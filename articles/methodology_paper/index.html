<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=false><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design &#183; JM Santiago III</title>
<meta name=title content="The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design &#183; JM Santiago III"><meta name=description content="Our conceptual framework integrates Generative AI into the live-prototyping process of User-Centered Design, transforming traditional methodologies by enabling real-time user feedback and AI-driven simulations to enhance design responsiveness and innovation."><meta name=keywords content="User-Centered Design,Generative AI,Live-Prototyping,AI in Design,"><link rel=canonical href=https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/><link type=text/css rel=stylesheet href=/hugo-portfolio/css/main.bundle.min.16ef8a6896d8151791996e6789da59a48d35efa92f465864bfebdef24f1b8a6b7b542a9f4a9ca1d2b987d1ac6c72f3ed184dab5eb1db16fa43b0da3255b9f86c.css integrity="sha512-Fu+KaJbYFReRmW5nidpZpI0176kvRlhkv+ve8k8bimt7VCqfSpyh0rmH0axscvPtGE2rXrHbFvpDsNoyVbn4bA=="><script type=text/javascript src=/hugo-portfolio/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/hugo-portfolio/js/main.bundle.min.b36dda9ec4ec11e967fd341e7d57b8c484ac7a39f8f329f7bcf7ce4812569de8607db866a086d4789956ac0b74967e251c9a566d94d469dd28b8787fed4f46f5.js integrity="sha512-s23ansTsEeln/TQefVe4xISsejn48yn3vPfOSBJWnehgfbhmoIbUeJlWrAt0ln4lHJpWbZTUad0ouHh/7U9G9Q==" data-copy data-copied></script><script src=/hugo-portfolio/js/zoom.min.js></script><link rel=apple-touch-icon sizes=180x180 href=/hugo-portfolio/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/hugo-portfolio/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/hugo-portfolio/favicon-16x16.png><link rel=manifest href=/hugo-portfolio/site.webmanifest><meta property="og:url" content="https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/"><meta property="og:site_name" content="JM Santiago III"><meta property="og:title" content="The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design"><meta property="og:description" content="Our conceptual framework integrates Generative AI into the live-prototyping process of User-Centered Design, transforming traditional methodologies by enabling real-time user feedback and AI-driven simulations to enhance design responsiveness and innovation."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2024-01-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-23T00:00:00+00:00"><meta property="article:tag" content="User-Centered Design"><meta property="article:tag" content="Generative AI"><meta property="article:tag" content="Live-Prototyping"><meta property="article:tag" content="AI in Design"><meta property="og:image" content="https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/featured.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/featured.png"><meta name=twitter:title content="The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design"><meta name=twitter:description content="Our conceptual framework integrates Generative AI into the live-prototyping process of User-Centered Design, transforming traditional methodologies by enabling real-time user feedback and AI-driven simulations to enhance design responsiveness and innovation."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Articles","name":"The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design","headline":"The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design","abstract":"Our conceptual framework integrates Generative AI into the live-prototyping process of User-Centered Design, transforming traditional methodologies by enabling real-time user feedback and AI-driven simulations to enhance design responsiveness and innovation.","inLanguage":"en","url":"https:\/\/Jamie03.github.io\/hugo-portfolio\/articles\/methodology_paper\/","author":{"@type":"Person","name":"Jose Maria Santiago III"},"copyrightYear":"2024","dateCreated":"2024-01-23T00:00:00\u002b00:00","datePublished":"2024-01-23T00:00:00\u002b00:00","dateModified":"2024-01-23T00:00:00\u002b00:00","keywords":["User-Centered Design","Generative AI","Live-Prototyping","AI in Design"],"mainEntityOfPage":"true","wordCount":"2980"}]</script><meta name=author content="Jose Maria Santiago III"><link href=mailto:jmsantiagoiii@gmail.com rel=me><link href=https://github.com/jamie03 rel=me><link href=https://www.linkedin.com/in/jmsantiagoiii/ rel=me><link href="https://orcid.org/my-orcid?orcid=0000-0002-4730-7865" rel=me><link href=https://www.researchgate.net/profile/Jose-Santiago-10 rel=me><script src=/hugo-portfolio/lib/jquery/jquery.slim.min.js integrity></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom bg-neutral dark:bg-neutral-800"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/hugo-portfolio/ class="text-base font-medium text-gray-500 hover:text-gray-900">JM Santiago III</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/hugo-portfolio/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Home</p></a><a href=/hugo-portfolio/articles/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Articles</p></a><a href=/hugo-portfolio/worlds/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Worlds</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/hugo-portfolio/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Home</p></a></li><li class=mt-1><a href=/hugo-portfolio/articles/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Articles</p></a></li><li class=mt-1><a href=/hugo-portfolio/worlds/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Worlds</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div class="w-full h-36 md:h-56 lg:h-72 single_hero_basic nozoom" style=background-image:url(/hugo-portfolio/articles/methodology_paper/featured_hu735e59f128e7c11a130d76ee61e227ae_1008017_1200x0_resize_box_3.png)></div><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">The AI of Oz: A Framework for Integrating Generative AI in User-Centered Design</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-01-23 00:00:00 +0000 UTC">23 January 2024</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">14 mins</span><span class="px-2 text-primary-500">&#183;</span>
<script type=text/javascript src=/hugo-portfolio/js/zen-mode.min.ccc8b2a0c4505216444ca25530c7328aa0bef94f43dabaf3e598e1b4271de9eec76c1cb35917471a4f7ccb8d84051b3acef42baa30724ac3a667fcf1fec4432a.js integrity="sha512-zMiyoMRQUhZETKJVMMcyiqC++U9D2rrz5ZjhtCcd6e7HbByzWRdHGk98y42EBRs6zvQrqjBySsOmZ/zx/sRDKg=="></script><span class=mb-[2px]><span id=zen-mode-button class="text-lg hover:text-primary-500" title="Enable zen mode" data-title-i18n-disable="Enable zen mode" data-title-i18n-enable="Disable zen mode"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50" height="50"><path fill="currentcolor" d="M12.980469 4C9.1204688 4 5.9804688 7.14 5.9804688 11L6 26H9.9804688V11c0-1.65 1.3400002-3 3.0000002-3H40.019531c1.66.0 3 1.35 3 3V39c0 1.65-1.34 3-3 3H29c0 1.54-.579062 2.94-1.539062 4H40.019531c3.86.0 7-3.14 7-7V11c0-3.86-3.14-7-7-7H12.980469zM7 28c-2.206.0-4 1.794-4 4V42c0 2.206 1.794 4 4 4H23c2.206.0 4-1.794 4-4V32c0-2.206-1.794-4-4-4H7zm0 4H23L23.001953 42H7V32z"/></svg></span></span></span></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/hugo-portfolio/tags/user-centered-design/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">User-Centered Design
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/hugo-portfolio/tags/generative-ai/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Generative AI
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/hugo-portfolio/tags/live-prototyping/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Live-Prototyping
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/hugo-portfolio/tags/ai-in-design/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI in Design</span></span></span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open class="toc-right mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#the-conceptual-framework>The Conceptual Framework</a><ul><li><a href=#adapting-the-wizard-of-oz-approach-with-generative-ai>Adapting the Wizard of Oz Approach with Generative AI</a></li><li><a href=#example-scenario-interactive-learning-application>Example Scenario: Interactive Learning Application</a></li><li><a href=#challenges-and-opportunities>Challenges and Opportunities</a></li></ul></li><li><a href=#the-case-study>The Case Study</a><ul><li><a href=#study-participants-and-setup>Study Participants and Setup</a></li><li><a href=#tasks-and-scenario>Tasks and Scenario</a></li><li><a href=#sample-scenario>Sample Scenario</a></li><li><a href=#mixed-feedback-and-niche-use-case>Mixed Feedback and Niche Use Case</a></li></ul></li><li><a href=#benefits-limitations-and-unexpected-findings>Benefits, Limitations, and Unexpected Findings</a><ul><li><a href=#potential-of-the-framework>Potential of the Framework</a></li><li><a href=#lack-of-flexibility-in-the-current-iteration>Lack of Flexibility in the Current Iteration</a></li><li><a href=#unexpected-applications-and-suggestions>Unexpected Applications and Suggestions</a></li><li><a href=#potential-problems-and-challenges>Potential Problems and Challenges</a></li></ul></li><li><a href=#integrating-generative-ai-into-user-centered-design>Integrating Generative AI into User-Centered Design</a><ul><li><a href=#the-learning-curve-and-barrier-to-entry>The Learning Curve and Barrier to Entry</a></li><li><a href=#reducing-barriers-with-user-friendly-ai-tools>Reducing Barriers with User-Friendly AI Tools</a></li><li><a href=#the-impact-of-user-friendly-generative-ai>The Impact of User-Friendly Generative AI</a></li></ul></li><li><a href=#future-directions-and-conclusion>Future Directions and Conclusion</a><ul><li><a href=#future-directions>Future Directions</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#the-conceptual-framework>The Conceptual Framework</a><ul><li><a href=#adapting-the-wizard-of-oz-approach-with-generative-ai>Adapting the Wizard of Oz Approach with Generative AI</a></li><li><a href=#example-scenario-interactive-learning-application>Example Scenario: Interactive Learning Application</a></li><li><a href=#challenges-and-opportunities>Challenges and Opportunities</a></li></ul></li><li><a href=#the-case-study>The Case Study</a><ul><li><a href=#study-participants-and-setup>Study Participants and Setup</a></li><li><a href=#tasks-and-scenario>Tasks and Scenario</a></li><li><a href=#sample-scenario>Sample Scenario</a></li><li><a href=#mixed-feedback-and-niche-use-case>Mixed Feedback and Niche Use Case</a></li></ul></li><li><a href=#benefits-limitations-and-unexpected-findings>Benefits, Limitations, and Unexpected Findings</a><ul><li><a href=#potential-of-the-framework>Potential of the Framework</a></li><li><a href=#lack-of-flexibility-in-the-current-iteration>Lack of Flexibility in the Current Iteration</a></li><li><a href=#unexpected-applications-and-suggestions>Unexpected Applications and Suggestions</a></li><li><a href=#potential-problems-and-challenges>Potential Problems and Challenges</a></li></ul></li><li><a href=#integrating-generative-ai-into-user-centered-design>Integrating Generative AI into User-Centered Design</a><ul><li><a href=#the-learning-curve-and-barrier-to-entry>The Learning Curve and Barrier to Entry</a></li><li><a href=#reducing-barriers-with-user-friendly-ai-tools>Reducing Barriers with User-Friendly AI Tools</a></li><li><a href=#the-impact-of-user-friendly-generative-ai>The Impact of User-Friendly Generative AI</a></li></ul></li><li><a href=#future-directions-and-conclusion>Future Directions and Conclusion</a><ul><li><a href=#future-directions>Future Directions</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>As someone deeply entrenched in the world of User-Centered Design (UCD), I have long appreciated the traditional methods that prioritize user feedback at every stage of the design process. However, I&rsquo;ve also experienced firsthand the slow pace and rigidity that often come with traditional prototyping and user studies. This led me to explore more dynamic and flexible approaches.</p><p>My journey into integrating Generative Artificial Intelligence (AI) with user studies began with a project focused on language learning. In this previous study, we utilized Generative AI to create personalized, scenario-based learning environments for second language acquisition. The project revealed the potential of Generative AI not just in adapting to user inputs on-the-fly but also in dramatically enhancing personalization and responsiveness.</p><p>These insights sparked a pivotal question: could the principles applied in language learning be extended to UCD to enhance prototype development? This led to the development of a conceptual framework aimed at revolutionizing UCD processes by incorporating live-prototyping with Generative AI. The goal was simple yet ambitious—to integrate real-time user feedback directly into the prototyping process, thereby shortening feedback loops and enhancing design agility.</p><p>This article details the evolution of this framework, the findings from its application, and the broader implications for the future of design and user interaction. Through sharing this journey, I hope to provide insights and inspiration for others in the field to consider how advanced AI tools could transform their own design methodologies.</p><h2 class="relative group">The Conceptual Framework<div id=the-conceptual-framework class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-conceptual-framework aria-label=Anchor>#</a></span></h2><p>The core of our conceptual framework involves integrating Generative AI into the live-prototyping process of User-Centered Design (UCD) in a manner that mirrors the traditional Wizard of Oz (WoZ) methodology. However, instead of human wizards simulating responses and functionalities behind the scenes, we employ Generative AI to perform these tasks in real-time based on dynamic user inputs.</p><img class=thumbnailshadow src="img/Live Prototyping Diagram 2.png" alt="The extended conceptual framework diagram of live-prototyping utilizing generative AI. Beginning at the bottom left, participants experience the prototype during study sessions by interacting with an interface, most commonly a GUI. Participants simultaneously communicate their feedback to the designer. Equipped with a control interface, the designer can manipulate various Generative AI instances, which then simulate integrated components in the interface experienced by the participant. This allows testing of new components and changes to existing ones."><h3 class="relative group">Adapting the Wizard of Oz Approach with Generative AI<div id=adapting-the-wizard-of-oz-approach-with-generative-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#adapting-the-wizard-of-oz-approach-with-generative-ai aria-label=Anchor>#</a></span></h3><p>In traditional WoZ setups, human operators respond to users&rsquo; actions in a controlled environment, simulating the responses of an undeveloped system. This approach allows researchers to gather user feedback on system design and functionality without fully developing the system. In our framework, we replace the human &ldquo;wizard&rdquo; with Generative AI, enabling a more scalable, flexible, and responsive prototyping environment.</p><p>Here’s how Generative AI is integrated into the live-prototyping process:</p><img class=thumbnailshadow src=img/UserViewWhite3.png alt="Screenshot of the users' interface of the language practicing tool. On the left, a contextual image, generated using a latent diffusion model. The top right features a chat window, showcasing past participant (``YOU:'') interactions with the AI conversational partner (``AI:''). The bottom right provides two input modalities - text or voice - used to converse with the AI."><ol><li><strong>Dual-Interface System</strong>: Similar to traditional setups, our system consists of two main interfaces:<ul><li><strong>User Interface (UI)</strong>: Where the user interacts with the prototype as if it were a fully functioning application.</li><li><strong>Designer Interface</strong>: Used by the designer to observe user interactions and control the Generative AI responses in real-time.</li></ul></li></ol><img class=thumbnailshadow src=img/DesignerViewWhite3.png alt="A screenshot of the designers' control interface during our study with prototypers, allowing for control of the language practicing tool. The left side displays the conversation flow, listing all of the implemented components and their order of execution. By clicking on each element, the designer can edit and add components to the tool. Further details can be seen on the right, such as a text input area for editing the name and buttons to add new functional elements.  A reset button sits at the bottom left, for restarting the tool's instance."><ol start=2><li><p><strong>Prompt-Based AI Simulation</strong>: At the core of the interaction is prompt-based simulation. Designers manipulate AI-generated prompts to simulate responses and features of the prototype based on real-time user feedback. This requires a deep understanding of how to craft effective prompts that guide the AI to produce the desired outcomes.</p></li><li><p><strong>Real-Time Response and Adaptation</strong>: As users interact with the prototype, they may encounter scenarios or request functionalities that are not yet fully developed. The designer, observing these interactions, can quickly adjust the AI’s prompts to simulate different responses or add new features temporarily. This immediate adaptation allows for testing multiple iterations and gathering rich user feedback without back-end developments.</p></li></ol><h3 class="relative group">Example Scenario: Interactive Learning Application<div id=example-scenario-interactive-learning-application class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#example-scenario-interactive-learning-application aria-label=Anchor>#</a></span></h3><p>Imagine a scenario where a user is interacting with an educational application designed to teach a new language. The user might request an interactive exercise that isn’t yet developed. Normally, this would require a return to the development phase, but with our AI-driven WoZ setup:</p><ul><li>The user requests the feature through the UI.</li><li>The designer sees this request on their interface and quickly crafts a prompt that instructs the AI to simulate an interactive language exercise.</li><li>The AI generates a temporary interface for the exercise based on the prompt, allowing the user to interact with this simulated feature immediately.</li><li>Feedback on this interaction is observed and used to refine the next iteration of the prototype.</li></ul><h3 class="relative group">Challenges and Opportunities<div id=challenges-and-opportunities class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#challenges-and-opportunities aria-label=Anchor>#</a></span></h3><p>While this AI-enhanced WoZ approach offers significant flexibility and speed in prototyping, it also presents challenges:</p><ul><li><strong>Prompt Precision</strong>: Crafting prompts that effectively communicate the desired AI actions can be complex, especially for designers who are more accustomed to visual design tools than textual AI interfaces.</li><li><strong>AI Understanding and Limitations</strong>: The AI might generate unexpected results if the prompt is ambiguous or if its capabilities are not aligned with the task, requiring careful oversight and quick adjustments from the designer.</li></ul><p>Despite these challenges, the potential of this framework to streamline the design process and integrate sophisticated AI capabilities into early-stage prototyping is immense. It opens up opportunities for more iterative, user-focused design processes where changes can be implemented and tested in real-time, significantly accelerating the design cycle and potentially leading to more innovative and user-aligned products.</p><h2 class="relative group">The Case Study<div id=the-case-study class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-case-study aria-label=Anchor>#</a></span></h2><p>To validate our conceptual framework, we designed a case study that would allow us to observe the live-prototyping process in action and gather concrete data on its effectiveness. The setup was meticulously designed to not just test the technical capabilities of the framework but also to deeply understand the participant experience, particularly from those assuming the role of designers in the study.</p><h3 class="relative group">Study Participants and Setup<div id=study-participants-and-setup class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#study-participants-and-setup aria-label=Anchor>#</a></span></h3><p>For the case study, we recruited a diverse group of individuals with varying degrees of experience in design and familiarity with AI tools. Importantly, these participants were placed in the role of the designer, tasked with using the conceptual framework to modify a prototype based on real-time feedback, which I provided in the role of the user. This role reversal was crucial, as it allowed participants to engage directly with the tools and methods proposed by our new framework, offering them a hands-on understanding of its potential and pitfalls.</p><h3 class="relative group">Tasks and Scenario<div id=tasks-and-scenario class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tasks-and-scenario aria-label=Anchor>#</a></span></h3><p>Each session followed a structured scenario designed to simulate a realistic application of the prototype under development. A typical session would proceed as follows:</p><ol><li><p><strong>Introduction to the Interface</strong>: Participants were first introduced to the dual-interface setup. They received a brief tutorial on how to use the designer interface to make live changes to the prototype.</p></li><li><p><strong>Live Interaction</strong>: As the user, I interacted with the prototype, performing tasks that were designed to test various aspects of its functionality. These tasks ranged from navigating through menus to entering data and using interactive elements.</p></li><li><p><strong>Feedback and Modification</strong>: During these interactions, I provided verbal feedback and suggestions for improvements. Participants were expected to use the Generative AI tools to implement changes immediately based on my feedback.</p></li><li><p><strong>Evaluation</strong>: After modifications were made, I would reassess the updated prototype, providing further feedback if needed. This iterative process was designed to mimic a rapid prototyping session where user feedback directly influences design decisions on the fly.</p></li></ol><h3 class="relative group">Sample Scenario<div id=sample-scenario class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sample-scenario aria-label=Anchor>#</a></span></h3><p>A sample scenario involved our prototype for a scenario-based language learning app that we worked on before. As the user, my task was to create different scenarios for my upcoming trip to Paris as I wanted to practice my french beforehand. I navigated the app, commenting on difficulties such as a lack of clear instruction on what to do, a missing translation feature, and configuring the image generation model to suit my tastes. As I voiced these issues, the participant-designer would use the framework to add guiding questions for making the scenario, simulating a click-to-translate feature by modifying the built-in prompt, and adjusting the parameters for the image generation model to address the feedback.</p><h3 class="relative group">Mixed Feedback and Niche Use Case<div id=mixed-feedback-and-niche-use-case class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mixed-feedback-and-niche-use-case aria-label=Anchor>#</a></span></h3><p>The feedback from participants was mixed, reflecting both the strengths and limitations of the live-prototyping approach. While some participants found it exhilarating to see their changes affect the user experience in real-time, others struggled with the pace and complexity of making on-the-spot modifications. Several designers noted that while the framework allowed for rapid iterations, it sometimes led to rushed decisions that might not hold up in a more prolonged evaluation.</p><p>This mixed response led us to realize that while powerful, our live-prototyping framework might best serve niche use cases where speed and agility are prioritized over depth and deliberation—such as in early-stage concept development or in environments where user feedback is highly unpredictable and diverse.</p><p>Further testing was recognized as essential to refine the framework and establish clearer guidelines for its application, ensuring that it can meet a broader range of design needs without overwhelming the designers or compromising the design quality. This case study, thus, marks the beginning of an ongoing exploration into the potential of integrating Generative AI into User-Centered Design processes.</p><h2 class="relative group">Benefits, Limitations, and Unexpected Findings<div id=benefits-limitations-and-unexpected-findings class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benefits-limitations-and-unexpected-findings aria-label=Anchor>#</a></span></h2><p>The exploration of our live-prototyping framework through this case study provided a multifaceted view of its potential, limitations, and unexpected applications. While the primary focus was on the integration of Generative AI into User-Centered Design, the feedback from participants offered insights into broader applications and highlighted several areas for improvement.</p><h3 class="relative group">Potential of the Framework<div id=potential-of-the-framework class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#potential-of-the-framework aria-label=Anchor>#</a></span></h3><p>The integration of Generative AI within our live-prototyping framework showcases significant potential to transform how designers interact with user feedback and iterate on prototypes. The ability to make real-time adjustments based on immediate user reactions can not only speed up the design process but also ensure that the end products are more closely aligned with user needs. This approach is particularly advantageous in dynamic environments where user requirements can change rapidly or are not fully known at the project&rsquo;s outset.</p><h3 class="relative group">Lack of Flexibility in the Current Iteration<div id=lack-of-flexibility-in-the-current-iteration class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lack-of-flexibility-in-the-current-iteration aria-label=Anchor>#</a></span></h3><p>Despite its benefits, the current iteration of the framework revealed a notable lack of flexibility in certain aspects. Participants noted that while the framework was effective for straightforward modifications (like adjusting text size or changing colors), it struggled with more complex changes that required deeper integration into the system’s logic or backend. For instance, modifying the logic of how the application responds to user inputs in a context-sensitive manner was not as intuitive or quick as adjusting visual elements. This limitation suggests a need for further development to enhance the framework’s ability to handle a wider range of modifications seamlessly.</p><h3 class="relative group">Unexpected Applications and Suggestions<div id=unexpected-applications-and-suggestions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#unexpected-applications-and-suggestions aria-label=Anchor>#</a></span></h3><p>Interestingly, participants saw potential for the framework beyond traditional user testing. Some suggested its use in brainstorming sessions or co-design processes where multiple stakeholders, including users, could suggest changes in real-time. This application could foster a more collaborative environment and potentially lead to more innovative solutions by involving diverse perspectives directly in the design process.</p><h3 class="relative group">Potential Problems and Challenges<div id=potential-problems-and-challenges class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#potential-problems-and-challenges aria-label=Anchor>#</a></span></h3><p>Participants also highlighted several potential problems with the framework:</p><ul><li><strong>Over-reliance on immediate feedback</strong>: There was a concern that the emphasis on instant modifications could lead to design choices that are reactionary rather than thoughtful and well-considered. This might result in features that work well in isolated instances but fail in a broader context or when used in conjunction with other features.</li><li><strong>Cognitive Load on Designers</strong>: The need to make quick decisions and implement changes immediately can be cognitively demanding for designers. This could lead to fatigue and reduce the quality of decision-making over prolonged sessions.</li><li><strong>Integration with Existing Tools</strong>: Participants expressed concerns about how well the live-prototyping tools would integrate with existing design and development workflows, which often involve a variety of platforms and technologies.</li></ul><p>Overall, the feedback from this case study paints a picture of a powerful, albeit currently limited, tool that holds promise for revolutionizing certain aspects of the design process. The potential to expand its application into areas like brainstorming and co-design is particularly exciting, suggesting a future where Generative AI not only facilitates but actively enhances creativity and collaboration within design teams. As we continue to refine the framework and address the challenges identified, we remain optimistic about its role in shaping the future of User-Centered Design.</p><h2 class="relative group">Integrating Generative AI into User-Centered Design<div id=integrating-generative-ai-into-user-centered-design class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#integrating-generative-ai-into-user-centered-design aria-label=Anchor>#</a></span></h2><p>The integration of Generative AI into User-Centered Design (UCD) represents a significant shift in design methodologies, promising to enhance the responsiveness and adaptability of design processes. However, the application of such advanced technologies is not without its challenges, particularly concerning the steep learning curve that can act as a barrier to entry for many designers.</p><h3 class="relative group">The Learning Curve and Barrier to Entry<div id=the-learning-curve-and-barrier-to-entry class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-learning-curve-and-barrier-to-entry aria-label=Anchor>#</a></span></h3><p>The use of Generative AI requires a solid understanding of both the technology itself and how it can be effectively applied within the framework of UCD. Designers must be adept not only at using traditional design tools but also at manipulating AI-driven components that respond to user inputs in real-time. This dual requirement can be daunting, particularly for professionals who may not have a background in AI or machine learning.</p><p>The initial barrier is significant because it involves both technical proficiency and a shift in mindset from traditional design processes to more dynamic, iterative approaches. Designers must learn to balance the immediacy of live changes with the need for thoughtful, strategic design decisions. This can be particularly challenging in fast-paced environments where the pressure to deliver immediate results may lead to rushed or suboptimal design choices.</p><h3 class="relative group">Reducing Barriers with User-Friendly AI Tools<div id=reducing-barriers-with-user-friendly-ai-tools class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#reducing-barriers-with-user-friendly-ai-tools aria-label=Anchor>#</a></span></h3><p>To realize the full potential of Generative AI in UCD, there is a pressing need to develop tools that are more intuitive and user-friendly. If AI tools can be designed to be more accessible, they could significantly lower the barrier to entry, allowing a broader range of designers to leverage this technology effectively. Here are a few ways that AI tools could be optimized for better integration into UCD:</p><ul><li><p><strong>Simplified Interfaces</strong>: AI tools with user-friendly interfaces that abstract the complexity of underlying AI processes can help designers focus more on design and less on managing technical details. For instance, interfaces that offer drag-and-drop components, visual programming environments, or preset AI behaviors could make it easier for designers to incorporate AI into their workflows.</p></li><li><p><strong>Integrated Learning Resources</strong>: Incorporating tutorials, guided workflows, and contextual help within AI tools can help designers learn on the go. This would make the learning process more organic and less intimidating, encouraging more designers to experiment with AI functionalities.</p></li><li><p><strong>Feedback-driven Development</strong>: Tools that incorporate feedback mechanisms to suggest optimizations or highlight potential improvements in real-time can aid designers in making more informed decisions quickly.</p></li></ul><h3 class="relative group">The Impact of User-Friendly Generative AI<div id=the-impact-of-user-friendly-generative-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-impact-of-user-friendly-generative-ai aria-label=Anchor>#</a></span></h3><p>If AI could be made more user-friendly, its potential impact on the design industry could be transformative. With easier access to and control of AI tools, designers could:</p><ul><li><p><strong>Enhance Creativity</strong>: By reducing the time spent on routine tasks, AI can free designers to focus on more creative aspects of design, such as exploring innovative user interaction models or experimenting with novel aesthetics.</p></li><li><p><strong>Increase Efficiency</strong>: Faster iteration cycles enabled by AI can shorten project timelines and increase the throughput of design teams, making the design process more agile and responsive to user needs.</p></li><li><p><strong>Improve Design Quality</strong>: With AI providing real-time data and feedback, designers can continuously refine and optimize their prototypes based on actual user interactions, potentially leading to higher-quality outcomes that are closely aligned with user expectations.</p></li></ul><p>While the adoption of Generative AI in User-Centered Design presents challenges, particularly in terms of the learning curve, its potential benefits are significant. By focusing on developing more user-friendly AI tools, we can help unlock these benefits and pave the way for a new era in design that is more creative, efficient, and responsive to user needs.</p><h2 class="relative group">Future Directions and Conclusion<div id=future-directions-and-conclusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions-and-conclusion aria-label=Anchor>#</a></span></h2><p>As we refine our conceptual framework for integrating Generative AI into User-Centered Design (UCD), the potential to revolutionize design processes is evident. The adaptation of the Wizard of Oz (WoZ) methodology using AI rather than human operators offers a blend of innovation and practicality that can address current and future design challenges. However, realizing the full potential of this approach requires addressing several key areas for development and improvement.</p><h3 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h3><ol><li><p><strong>Enhanced Prompt Engineering Tools</strong>: To mitigate the complexity of crafting effective AI prompts, development of more intuitive prompt engineering tools is essential. These tools could offer real-time suggestions, corrections, and even automated prompt generation based on the designer&rsquo;s input and user interactions, making the technology more accessible to designers without deep AI expertise.</p></li><li><p><strong>Training and Education</strong>: As the reliance on textual AI interfaces increases, there is a growing need for specialized training programs for designers. These programs should focus not only on the technical aspects of Generative AI but also on how to integrate AI tools seamlessly into traditional design workflows.</p></li><li><p><strong>Expanding AI Capabilities</strong>: To further the effectiveness of the AI-driven WoZ approach, continuous improvement of AI models is required. This includes enhancing their ability to understand complex user inputs, generate more accurate and contextually appropriate responses, and handle a wider range of simulation tasks without designer intervention.</p></li><li><p><strong>Cross-Disciplinary Collaboration</strong>: Collaborative efforts between AI researchers, UX designers, and industry practitioners will be vital. Such collaborations can ensure that the developments in AI technologies are aligned with the practical needs and challenges of real-world design processes.</p></li><li><p><strong>Ethical and Practical Considerations</strong>: As AI takes on more responsibilities in simulating user interactions, addressing ethical concerns such as data privacy, user consent, and transparency becomes crucial. Additionally, practical considerations, including the integration of AI tools with existing software and systems, must be addressed to ensure smooth deployment and operation.</p></li></ol><h3 class="relative group">Conclusion<div id=conclusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion aria-label=Anchor>#</a></span></h3><p>The integration of Generative AI into the live-prototyping process presents a promising frontier for User-Centered Design. By enabling real-time, adaptable simulations through AI-driven responses, this approach not only accelerates the design process but also enhances its responsiveness to user needs. Our conceptual framework, while still in need of refinement, illustrates a clear path toward more dynamic and innovative design methodologies.</p><p>This approach could particularly benefit projects where user feedback is critical and where design requirements are likely to evolve rapidly. By reducing the time and resources needed to iterate on designs, we enable designers to focus more on creative solutions and user satisfaction. Ultimately, as we continue to develop and refine this framework, our goal is to provide a robust toolset that empowers designers to harness the full potential of Generative AI in crafting user-centered solutions.</p></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/&amp;text=The%20AI%20of%20Oz:%20A%20Framework%20for%20Integrating%20Generative%20AI%20in%20User-Centered%20Design" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/&amp;resubmit=true&amp;title=The%20AI%20of%20Oz:%20A%20Framework%20for%20Integrating%20Generative%20AI%20in%20User-Centered%20Design" title="Submit to Reddit" aria-label="Submit to Reddit"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://Jamie03.github.io/hugo-portfolio/articles/methodology_paper/&amp;subject=The%20AI%20of%20Oz:%20A%20Framework%20for%20Integrating%20Generative%20AI%20in%20User-Centered%20Design" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_articles/methodology_paper/index.md",oid_likes="likes_articles/methodology_paper/index.md"</script><script type=text/javascript src=/hugo-portfolio/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/hugo-portfolio/articles/research_project/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Utilizing Generative AI for Scenario-Based Second Language Acquisition</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-06-10 00:00:00 +0000 UTC">10 June 2023</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/hugo-portfolio/articles/industry_project/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Affective State Change via Procedurally Generated Haptics</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-01-23 00:00:00 +0000 UTC">23 January 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">© 2024 JM Santiago. All Rights Reserved.</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/hugo-portfolio/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://Jamie03.github.io/hugo-portfolio/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>